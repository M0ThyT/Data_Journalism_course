{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping with Selenium\n",
    "In this notebook I will give a short explanation of how you can use Selenium for webscraping. I think one of the big advantages of Selenium is how intuitive it is. You can write a script that does things exactly the way you would do it by hand, but then you can scale it up and run it one much more websites than you could do by hand in a similar time. In this Notebook I will go throug two short examples, the first one is a small scraper that gives you the weekly new Covid infection rates as published by the RIVM, the second scrapes the name of all senators of the United States of America. \n",
    "\n",
    "However, first let's start by loading the relevant packages for the script, and by setting a working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Packages\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from fake_useragent import UserAgent\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "#Working directory\n",
    "os.chdir('/home/timothy/Desktop/University/1Studium/Communication Science/Kurse/s3/Data Journalism')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RIVM\n",
    "The RIVM is the Dutch National Institute for Public Health and Environment and thus has credible covid numbers. Let's say we are interested in those, and usually we check the website daily. But now, we don't have a lot of time anymore and we would like to automate the process so that we just have to run our script, and then we get the number in return. \n",
    "\n",
    "Let's start by saving the starting URL as a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.rivm.nl/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will set up our browser, and send the first get request to the url we have specified. The most important part in the code below is the executable path you need for the browser for it to be able to locate the geckodrive exe file you need to download on your laptop. If everything works, a new mozilla firefox window should appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = webdriver.FirefoxProfile()\n",
    "options = Options()         \n",
    "#options.headless = True \n",
    "ua = UserAgent(verify_ssl=False)\n",
    "options.add_argument('--disable-infobars')\n",
    "options.add_argument('--disable-extensions')\n",
    "options.add_argument('--profile-directory=Default')\n",
    "options.add_argument('--incognito')\n",
    "options.add_argument('--disable-plugins-discovery')\n",
    "options.add_argument('--start-maximized')\n",
    "userAgent = ua.random\n",
    "options.add_argument(f'user-agent={userAgent}')\n",
    "browser = webdriver.Firefox(options = options,firefox_profile=profile, executable_path = r'presentation/geckodriver.exe' ) #this is quite important \n",
    "wait = WebDriverWait(browser, 300)  \n",
    "browser.get(url) # open url   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the website is open we will use the xpath notation to help the computer identify which link to click on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.implicitly_wait(20) # in case the website doesn't load fast enough, this tells the laptop to wait up to 20 seconds if it cannot immediatly execute the next commands\n",
    "browser.find_element_by_xpath('/html/body/div[2]/div/div[3]/main/div[2]/article/div/div/div/div/div/div[2]/div[2]/div/div/div/div/div/ul/li[1]/a').click() #this is the link we want to click on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are on the correct website, we will write some code to scroll down the website so the data we are interested in is in our field of vision, then we will use the x-path notation again to tell the computer which number we are interested in. At this point we will be done so we will close the browser window, and print out a statement that tells us how many new weekly infections we have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. There were 36.931 new cases last week\n"
     ]
    }
   ],
   "source": [
    "browser.execute_script(\"window.scrollTo(0, 1000);\") #scrolling down\n",
    "\n",
    "#this are the numbers we are interested in\n",
    "new_cases_last_week = browser.find_element_by_xpath('/html/body/div[2]/div/div[3]/main/div[2]/article/div/div[2]/div/div/div/div[2]/div/table/tbody/tr[2]/td[2]').text\n",
    "\n",
    "#closing the browser\n",
    "browser.quit()\n",
    "\n",
    "#print out what we want to know\n",
    "print(f'There were {new_cases_last_week} new cases last week')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## US Senators\n",
    "In the second example, we will use Selenium to scrape the US Senators. Lets start again with specifying the URL and starting the browser. We will also create empty lists in which we can later safe the information in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.senate.gov/senators/index.htm'\n",
    "###Settings for the browser\n",
    "profile = webdriver.FirefoxProfile()\n",
    "options = Options()         \n",
    "#options.headless = True \n",
    "ua = UserAgent(verify_ssl=False)\n",
    "options.add_argument('--disable-infobars')\n",
    "options.add_argument('--disable-extensions')\n",
    "options.add_argument('--profile-directory=Default')\n",
    "options.add_argument('--incognito')\n",
    "options.add_argument('--disable-plugins-discovery')\n",
    "options.add_argument('--start-maximized')\n",
    "userAgent = ua.random\n",
    "options.add_argument(f'user-agent={userAgent}')\n",
    "browser = webdriver.Firefox(options = options,firefox_profile=profile, executable_path = r'presentation/geckodriver.exe' ) #this is quite important \n",
    "wait = WebDriverWait(browser, 300)  \n",
    "browser.get(url) # open url   \n",
    "\n",
    "#empty lists in which we will safe the data\n",
    "Name = []\n",
    "State = []\n",
    "Party = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will scroll down the page so we can see the actual table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scroll down the page\n",
    "browser.execute_script(\"window.scrollTo(0, 700);\")\n",
    "browser.implicitly_wait(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunatly, there are too many different names so they don't all fit on the window we see. In selenium, this will lead to issues because the scraper can only get data it can see. So lets reduce teh amount of senators we see at once from 100 to 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select 10 per page\n",
    "browser.find_element_by_xpath('//select[@aria-controls = \"listOfSenators\"]').click()\n",
    "browser.find_element_by_xpath('//select[@aria-controls = \"listOfSenators\"]/option').click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will have to extract all the information from this table that we are interested in. However, since we only see 10 names per page we will have to extract information the same way multiple times, so lets write a function that does this for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getting_senators():\n",
    "    Senators_o = browser.find_elements_by_xpath('//tr[@class = \"odd\"]')\n",
    "    \n",
    "    for senator in Senators_o: \n",
    "        Name.append(senator.find_element_by_xpath('./td/a').text)\n",
    "        State.append(senator.find_element_by_xpath('./td[2]').text)\n",
    "        Party.append(senator.find_element_by_xpath('./td[3]').text)\n",
    "    \n",
    "    \n",
    "    #even\n",
    "    Senators_e = browser.find_elements_by_xpath('//tr[@class = \"even\"]')\n",
    "    \n",
    "    for senator in Senators_e: \n",
    "        Name.append(senator.find_element_by_xpath('./td/a').text)\n",
    "        State.append(senator.find_element_by_xpath('./td[2]').text)\n",
    "        Party.append(senator.find_element_by_xpath('./td[3]').text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets start applying this this function to the first 3 pages and then quit the browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract info\n",
    "getting_senators()  \n",
    "#click next\n",
    "browser.find_element_by_xpath('//a[@id = \"listOfSenators_next\"]').click()\n",
    "#wait 2 secondswhi\n",
    "time.sleep(2)\n",
    "getting_senators()\n",
    "browser.find_element_by_xpath('//a[@id = \"listOfSenators_next\"]').click()\n",
    "time.sleep(2)\n",
    "getting_senators()\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we will do is put the 3 lists together into one pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 30 Senators in our dataset\n",
      "                       Name      State       Party\n",
      "0   Alexander, Lamar (R-TN)  Tennessee  Republican\n",
      "1     Barrasso, John (R-WY)    Wyoming  Republican\n",
      "2  Blackburn, Marsha (R-TN)  Tennessee  Republican\n",
      "3         Blunt, Roy (R-MO)   Missouri  Republican\n",
      "4      Boozman, John (R-AR)   Arkansas  Republican\n"
     ]
    }
   ],
   "source": [
    "Senators = pd.DataFrame(list(zip(Name, State, Party)),\n",
    "                  columns =['Name', 'State', 'Party']) \n",
    "\n",
    "print(f'There are {len(Senators)} Senators in our dataset')\n",
    "print(Senators.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale up and doing it headless\n",
    "we sucessfully scraped the first 30 senators. But usually when we scrape, we want all the information, so let's scale it up to all 100 senators. Furthermore, while watching the scraper work is nice for demonstration purposes, usually it is a lot easier to do it invisibly in the background. So what we will do this time, is to add the option headless = true. Further, instead of copy pasting the same data extraction commands multiple times, we will simply put it in a loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping complete\n"
     ]
    }
   ],
   "source": [
    "#url\n",
    "url = 'https://www.senate.gov/senators/index.htm'\n",
    "#settings for the browser\n",
    "profile = webdriver.FirefoxProfile()\n",
    "options = Options()         \n",
    "options.headless = True  #this is the line that makes it now invisible\n",
    "ua = UserAgent(verify_ssl=False)\n",
    "options.add_argument('--disable-infobars')\n",
    "options.add_argument('--disable-extensions')\n",
    "options.add_argument('--profile-directory=Default')\n",
    "options.add_argument('--incognito')\n",
    "options.add_argument('--disable-plugins-discovery')\n",
    "options.add_argument('--start-maximized')\n",
    "userAgent = ua.random\n",
    "options.add_argument(f'user-agent={userAgent}')\n",
    "browser = webdriver.Firefox(options = options,firefox_profile=profile, executable_path = r'presentation/geckodriver.exe' ) #this is quite important \n",
    "wait = WebDriverWait(browser, 300)  \n",
    "browser.get(url) # open url \n",
    "#empty lists\n",
    "Name = []\n",
    "State = []\n",
    "Party = []\n",
    "\n",
    "\n",
    "#scroll down the page\n",
    "browser.implicitly_wait(20)\n",
    "browser.execute_script(\"window.scrollTo(0, 700);\")\n",
    "\n",
    "#select 10 per page\n",
    "browser.find_element_by_xpath('//select[@aria-controls = \"listOfSenators\"]').click()\n",
    "browser.find_element_by_xpath('//select[@aria-controls = \"listOfSenators\"]/option').click()\n",
    "\n",
    "#define the data extraction function\n",
    "def getting_senators():\n",
    "    Senators_o = browser.find_elements_by_xpath('//tr[@class = \"odd\"]')\n",
    "    \n",
    "    for senator in Senators_o: \n",
    "        Name.append(senator.find_element_by_xpath('./td/a').text)\n",
    "        State.append(senator.find_element_by_xpath('./td[2]').text)\n",
    "        Party.append(senator.find_element_by_xpath('./td[3]').text)\n",
    "    \n",
    "    \n",
    "    #even\n",
    "    Senators_e = browser.find_elements_by_xpath('//tr[@class = \"even\"]')\n",
    "    \n",
    "    for senator in Senators_e: \n",
    "        Name.append(senator.find_element_by_xpath('./td/a').text)\n",
    "        State.append(senator.find_element_by_xpath('./td[2]').text)\n",
    "        Party.append(senator.find_element_by_xpath('./td[3]').text)\n",
    "\n",
    "\n",
    "#write a for loop that repeats the same steps 10 times (and clicks next 9 times)\n",
    "x = 0\n",
    "while x < 10:\n",
    "    getting_senators()\n",
    "    if x < 9: \n",
    "        browser.find_element_by_xpath('//a[@id = \"listOfSenators_next\"]').click()\n",
    "    else: \n",
    "        print('Scraping complete')\n",
    "    \n",
    "    x += 1\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "#store in Pandas dataframe\n",
    "Senators_data = pd.DataFrame(list(zip(Name, State, Party)),\n",
    "                  columns =['Name', 'State', 'Party']) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets look at our Dataset again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 100 Senators in our Dataset, 53 are Republicans and 45 are Democrats.\n",
      "                         Name        State       Party\n",
      "0     Alexander, Lamar (R-TN)    Tennessee  Republican\n",
      "1       Barrasso, John (R-WY)      Wyoming  Republican\n",
      "2    Blackburn, Marsha (R-TN)    Tennessee  Republican\n",
      "3           Blunt, Roy (R-MO)     Missouri  Republican\n",
      "4        Boozman, John (R-AR)     Arkansas  Republican\n",
      "5       Baldwin, Tammy (D-WI)    Wisconsin    Democrat\n",
      "6   Bennet, Michael F. (D-CO)     Colorado    Democrat\n",
      "7  Blumenthal, Richard (D-CT)  Connecticut    Democrat\n",
      "8      Booker, Cory A. (D-NJ)   New Jersey    Democrat\n",
      "9          Braun, Mike (R-IN)      Indiana  Republican\n"
     ]
    }
   ],
   "source": [
    "#lets also create seperate dfs for republicans and democrats\n",
    "republicans = Senators_data[Senators_data['Party'] == 'Republican']\n",
    "democrats = Senators_data[Senators_data['Party'] == 'Democrat']\n",
    "print(f'There are {len(Senators_data)} Senators in our Dataset, {len(republicans)} are Republicans and {len(democrats)} are Democrats.')\n",
    "\n",
    "print(Senators_data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
